{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c43acf52",
   "metadata": {},
   "source": [
    "# Staff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568de57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_staff = pd.read_csv('data/annotations/staff_annotations.csv', sep=';').dropna()\n",
    "data_staff.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba554675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "# Compute Cohen's Kappa using the cohen_kappa_score function from scikit-learn\n",
    "kappa = cohen_kappa_score(data_staff['nlabel_staff1'], data_staff['nlabel_staff2'])\n",
    "\n",
    "print(f\"Pairs len {len(data_staff)}\")\n",
    "print(kappa)\n",
    "\n",
    "\n",
    "print(f\"Pairs with agreement {len(data_staff[data_staff.nlabel_staff1 == data_staff.nlabel_staff2])} --> {round(len(data_staff[data_staff.nlabel_staff1 == data_staff.nlabel_staff2])/len(data_staff), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc7d5e",
   "metadata": {},
   "source": [
    "# Crowdsourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mturk = pd.read_csv('data/annotations/mturk_annotations.csv', sep=';').set_index(\n",
    "    [\"set_id\", \"ver_id\", \"reference_yt_id\", \"candidate_yt_id\", \"sample_group\"])\n",
    "data_mturk.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fleiss_kappa_pivot(df_hit_pivot: pd.DataFrame):\n",
    "    from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "\n",
    "    try:\n",
    "        arr, categories = aggregate_raters(df_hit_pivot.dropna().astype('int64'), 4)\n",
    "        return round(fleiss_kappa(arr), 2)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def krippendorff_pivot(df_hit_pivot: pd.DataFrame):\n",
    "    import krippendorff\n",
    "    df_hit_pivot = df_hit_pivot[[f\"worker_ind{i}\" for i in range(5)]]\n",
    "    try:\n",
    "        return round(krippendorff.alpha(df_hit_pivot.T, level_of_measurement='ordinal'), 2)\n",
    "    except AssertionError:\n",
    "        return None\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "krippendorff_pivot(data_mturk)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94f743f6",
   "metadata": {},
   "source": [
    "# Evaluation of MTurk against Staff\n",
    "Here we evaluate MTurk annotations using staff annotations as ground truth.\n",
    "First, we load the file and aggregate by majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mturk_staff = pd.read_csv('data/annotations/mturk_staff.csv', sep=';')\n",
    "\n",
    "def attach_mv_labels(data):\n",
    "    \n",
    "    from scipy import stats\n",
    "    \n",
    "    # filter MTurk vote columns\n",
    "    data_votes = data[[\"worker_ind\" + str(i) for i in range(5)]]\n",
    "    \n",
    "    mode, count = stats.mode(data_votes.T)\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    data[\"mv_nlabel\"] = np.where(count[0] > 2, mode[0], np.nan)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data_mturk_staff = attach_mv_labels(data_mturk_staff)\n",
    "\n",
    "data_mturk_staff = pd.merge(data_mturk_staff.iloc[:,1:], data_staff[[\"set_id\", \"candidate_yt_id\", \"nlabel_staff1\", \"nlabel_staff2\"]], \n",
    "        on=[\"set_id\", \"candidate_yt_id\"], how='left')\n",
    "\n",
    "data_mturk_staff = data_mturk_staff.query(\"~nlabel_staff1.isna() & ~nlabel_staff2.isna()\")\n",
    "\n",
    "print(f\"Videos all: {len(data_mturk_staff)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = krippendorff_pivot(data_mturk_staff[\n",
    "    [\"worker_ind0\", \"worker_ind1\", \"worker_ind2\", \"worker_ind3\", \"worker_ind4\"]])\n",
    "\n",
    "\n",
    "print(f\"Pairs len {len(data_mturk_staff)}\")\n",
    "print(f\"alpha {alpha}\")\n",
    "kappa = cohen_kappa_score(data_mturk_staff[\"nlabel_staff1\"], data_mturk_staff[\"nlabel_staff2\"])\n",
    "print(f\"kappa {kappa}\")\n",
    "\n",
    "\n",
    "print(f\"with MV Label {len(data_mturk_staff.query('~mv_nlabel.isna()'))}\")\n",
    "print(f\"with MV Label {round(len(data_mturk_staff.query('~mv_nlabel.isna()'))/len(data_mturk_staff), 2)}\")\n",
    "\n",
    "data_mturk_staff_with_mv = data_mturk_staff.query('~mv_nlabel.isna()')\n",
    "kappa = cohen_kappa_score(data_mturk_staff_with_mv[\"mv_nlabel\"], data_mturk_staff_with_mv[\"nlabel_staff\"])\n",
    "\n",
    "print(f\"Kappa MV Label and Students: {round(kappa, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/SHS-YT.csv\", sep=';')\n",
    "\n",
    "def rename_sample_groups(x):\n",
    "    if x != x: # this works to detect nans\n",
    "        return x\n",
    "    else:\n",
    "        return x.replace(\"re-move_favs\", \n",
    "                         \"DisAgrMus\").replace(\"ditto_favs\", \n",
    "                                     \"DisAgrTxt\").replace(\"mutual_unconfident\", \"MutUnc\")\n",
    "\n",
    "# Create a list of worker columns\n",
    "worker_cols = ['worker_ind0', 'worker_ind1', 'worker_ind2', 'worker_ind3', 'worker_ind4']\n",
    "\n",
    "# Create a new dataframe with the relevant columns\n",
    "pivot_data = data[worker_cols + ['sample_group', 'label']]\n",
    "\n",
    "\n",
    "# Group by 'sample_group' and 'mv_nlabel' and count occurrences\n",
    "pivot_table = pivot_data.groupby(['sample_group', 'label']).size().unstack(fill_value=0)\n",
    "\n",
    "\n",
    "custom_sort_order = ['Match', 'Version', 'Other', 'No Music']\n",
    "\n",
    "# Reindex the DataFrame based on the custom sort order\n",
    "pivot_table = pivot_table.reindex(columns=custom_sort_order)\n",
    "\n",
    "  \n",
    "for sample_group in data.sample_group.unique():\n",
    "    pivot_table.loc[sample_group, \"Agreement\"] = krippendorff_pivot(data.query(f\"sample_group == '{sample_group}'\"))\n",
    "    pivot_table.loc[sample_group, \"Expert Curated\"] = int(data.query(f\"sample_group == '{sample_group}'\").label_expert.count())\n",
    "    pivot_table.loc[sample_group, \"Author Re-Annotated\"] = int(data.query(f\"sample_group == '{sample_group}' & (nlabel_expert != nlabel_mturk) & ~nlabel_expert.isna()\").label_expert.count())\n",
    "\n",
    "\n",
    "pivot_table.index = pivot_table.index.to_series().apply(rename_sample_groups).fillna(\"None\")\n",
    "pivot_table = pivot_table.rename_axis(\"Group\").rename_axis(\"Label\", axis=1)\n",
    "pivot_table\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shs-yt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
