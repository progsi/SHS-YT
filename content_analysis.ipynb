{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Analysis\n",
    "We aim to analyze drivers of uncertainty. For the whole annotated set, we compute the Pearson correlation coefficient between following attributes and the mean similarity of version candidates to their query items:\n",
    "- *viewcount* as a proxy for popularity\n",
    "- *length* to identify whether shorter items tend to be identified worse than longer items\n",
    "- *music ratio* estimated by [YOHO](https://github.com/satvik-venkatesh/you-only-hear-once)\n",
    "\n",
    "We further analyze whether specific cues indicate more difficult candidates. Lastly, an inhouse export and one corresponding author manually curated some samples from the set and documented potential drivers of uncertainty. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import utils\n",
    "\n",
    "data_seed = pd.read_csv(\"data/metadata/seed_shs100k.csv\", sep=\";\")[\n",
    "    [\"set_id\", \"title\", \"performer\"]].drop_duplicates(subset=\"set_id\")\n",
    "data_ch, ytrue_ch, ypred_ch = utils.get_dataset(\"coverhunter\", \"SHS-SEED+YT\")\n",
    "data_cqt, ytrue_cqt, ypred_cqt = utils.get_dataset(\"cqtnet\", \"SHS-SEED+YT\")\n",
    "\n",
    "# merge yt metadata\n",
    "data_ch = pd.merge(data_ch, pd.read_hdf(\n",
    "    \"data/metadata/yt_metadata.h5\").reset_index(), \n",
    "                on=\"yt_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# limit to queries by Seed items only: CoverHunter\n",
    "ytrue_seedq_ch = ytrue_ch[data_ch.seed.values]\n",
    "ypred_seedq_ch = ypred_ch[data_ch.seed.values]\n",
    "\n",
    "rel_matrix_ch = utils.csi_relationship_matrix(data_ch)\n",
    "rel_matrix_seedq_ch = rel_matrix_ch[data_ch.seed.values]\n",
    "\n",
    "# limit to queries by Seed items only: CQTNet\n",
    "ytrue_seedq_cqt = ytrue_cqt[data_cqt.seed.values]\n",
    "ypred_seedq_cqt = ypred_cqt[data_cqt.seed.values]\n",
    "\n",
    "rel_matrix_cqt = utils.csi_relationship_matrix(data_cqt)\n",
    "rel_matrix_seedq_cqt = rel_matrix_cqt[data_cqt.seed.values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Negatives: What is hard to find and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# compute column wise means, CoverHunter\n",
    "sims_cols_ch = torch.where(ytrue_seedq_ch == 0, torch.nan, ypred_seedq_ch)\n",
    "data_ch[\"mean_sim_ch\"] = torch.nanmean(sims_cols_ch, dim=0)\n",
    "data_ch[[\"set_id\", \"yt_id\", \"mean_sim_ch\", \"sample_group\", \"nlabel\", 'title', 'viewcount', 'duration']].sort_values(by=\"mean_sim_ch\") #[\"mean_sim\"]\n",
    "\n",
    "# compute column wise means, CQTNet\n",
    "sims_cols_cqt = torch.where(ytrue_seedq_cqt == 0, torch.nan, ypred_seedq_cqt)\n",
    "data_cqt[\"mean_sim_cqt\"] = torch.nanmean(sims_cols_cqt, dim=0)\n",
    "\n",
    "# both CSI models\n",
    "data = pd.merge(data_ch, data_cqt[[\"set_id\", \"yt_id\", \"mean_sim_cqt\"]], how=\"left\", \n",
    "         on=[\"set_id\", \"yt_id\"])\n",
    "data = data[['set_id', 'reference_yt_id', 'yt_id', 'seed', 'sample_group',\n",
    "       'label', 'mean_sim_ch', 'mean_sim_cqt', 'title', 'viewcount', 'duration',\n",
    "       'origin', 'description', 'upload_date', 'channel_name', 'ditto_pred', \n",
    "       're-move_pred', 'nlabel']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duration and viewcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "\n",
    "print(\"CoverHunter: duration and mean similarity\")\n",
    "data_cor = data.query(\"nlabel >= 2\").dropna(subset=\"mean_sim_ch\")\n",
    "r, p = stats.pearsonr(data_cor.duration, data_cor.mean_sim_ch)\n",
    "print(f\"Pearson correlation {r:.2f} {p:.2f} \\n\")\n",
    "\n",
    "print(\"CQTNet: duration and mean similarity\")\n",
    "data_cor = data.query(\"nlabel >= 2\").dropna(subset=\"mean_sim_cqt\")\n",
    "r, p = stats.pearsonr(data_cor.duration, data_cor.mean_sim_cqt)\n",
    "print(f\"Pearson correlation {r:.2f} {p:.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation viewcount and mean similarity\")\n",
    "data_cor = data.query(\"nlabel >= 2\").dropna(subset=\"mean_sim_ch\")\n",
    "r, p = stats.pearsonr(data_cor.viewcount, data_cor.mean_sim_ch)\n",
    "print(f\"Pearson correlation {r:.2f} {p:.2f} \\n \")\n",
    "\n",
    "print(\"CQTNet: viewcount and mean similarity\")\n",
    "data_cor = data.query(\"nlabel >= 2\").dropna(subset=\"mean_sim_cqt\")\n",
    "r, p = stats.pearsonr(data_cor.viewcount, data_cor.mean_sim_cqt)\n",
    "print(f\"Pearson correlation {r:.2f} {p:.2f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions of Mean Similarities for different Label Origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.kdeplot(data=data.query(\"origin != 'seed' and origin != 'staff'\"), x=\"mean_sim_ch\", hue=\"origin\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.scatterplot(data=data, x=\"mean_sim_cqt\", y=\"mean_sim_ch\", hue=\"sample_group\")\n",
    "plt.xlabel(\"CQTNet Cosine Similarity\")\n",
    "plt.ylabel(\"CoverHunter Cosine Similarity\")\n",
    "\n",
    "plt.title(\"Versions: Sample Groups vs. CSI Benchmark Models\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Cues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CoverHunter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cue map\n",
    "cue_map = pd.read_csv(\"data/cues/cue_map.csv\", sep=\";\")\n",
    "cue_map\n",
    "\n",
    "\n",
    "#merge to data versions\n",
    "cue_cols = cue_map.columns[1:]\n",
    "data_version_cues = pd.merge(data.loc[data.nlabel > 1, [\"set_id\", \"yt_id\", \"sample_group\", \"nlabel\", \n",
    "                           \"mean_sim_ch\", \"mean_sim_cqt\"]], cue_map, \n",
    "         on=\"yt_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# Create an empty dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Iterate through each boolean column\n",
    "for bool_col in cue_cols:\n",
    "    selected_rows = data_version_cues[data_version_cues[bool_col]]  # Select rows where bool column is True\n",
    "    mean_mean_sim = selected_rows['mean_sim_ch'].mean()\n",
    "    std_dev_mean_sim = selected_rows['mean_sim_ch'].std()\n",
    "    support = selected_rows['mean_sim_ch'].sum()\n",
    "\n",
    "    results[bool_col] = {'mean': mean_mean_sim, 'std_dev': std_dev_mean_sim, 'support': support}\n",
    "    \n",
    "    \n",
    "pd.DataFrame(results).T.sort_values(by=\"support\", ascending=False).head(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CQTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cue map\n",
    "cue_map = pd.read_csv(\"data/cues/cue_map.csv\", sep=\";\")\n",
    "cue_map\n",
    "\n",
    "\n",
    "#merge to data versions\n",
    "cue_cols = cue_map.columns[1:]\n",
    "data_version_cues = pd.merge(data.loc[data.nlabel > 1, [\"set_id\", \"yt_id\", \"sample_group\", \"nlabel\", \n",
    "                           \"mean_sim_ch\", \"mean_sim_cqt\"]], cue_map, \n",
    "         on=\"yt_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# Create an empty dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Iterate through each boolean column\n",
    "for bool_col in cue_cols:\n",
    "    selected_rows = data_version_cues[data_version_cues[bool_col]]  # Select rows where bool column is True\n",
    "    mean_mean_sim = selected_rows['mean_sim_cqt'].mean()\n",
    "    std_dev_mean_sim = selected_rows['mean_sim_cqt'].std()\n",
    "    support = selected_rows['mean_sim_cqt'].sum()\n",
    "\n",
    "    results[bool_col] = {'mean': mean_mean_sim, 'std_dev': std_dev_mean_sim, 'support': support}\n",
    "    \n",
    "    \n",
    "pd.DataFrame(results).T.sort_values(by=\"support\", ascending=False).head(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_cols_ch = torch.where(torch.tensor(rel_matrix_seedq_ch == 'yt-neg'), ypred_seedq_ch, torch.nan)\n",
    "\n",
    "data_ch[\"mean_sim_neg_ch\"] = torch.nanmean(sims_cols_ch, dim=0)\n",
    "\n",
    "# compute column wise means, CQTNet\n",
    "sims_cols_cqt = torch.where(torch.tensor(rel_matrix_seedq_cqt == 'yt-neg'), ypred_seedq_cqt, torch.nan)\n",
    "data_cqt[\"mean_sim_neg_cqt\"] = torch.nanmean(sims_cols_cqt, dim=0)\n",
    "\n",
    "# both CSI models\n",
    "data_neg = pd.merge(data_ch, data_cqt[[\"set_id\", \"yt_id\", \"mean_sim_neg_cqt\"]], how=\"left\", \n",
    "         on=[\"set_id\", \"yt_id\"])\n",
    "data_neg = data_neg[['set_id', 'reference_yt_id', 'yt_id', 'seed', 'sample_group',\n",
    "       'label', 'mean_sim_neg_ch', 'mean_sim_neg_cqt', 'mean_sim_ch', 'title', 'viewcount', 'duration',\n",
    "       'origin', 'description', 'upload_date', 'channel_name', 'ditto_pred', \n",
    "       're-move_pred', 'nlabel']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data_neg, x=\"mean_sim_neg_cqt\", y=\"mean_sim_neg_ch\", hue=\"sample_group\")\n",
    "plt.title(\"Non-Versions: Sample Groups vs. CSI Benchmark Models\")\n",
    "plt.xlabel(\"CQTNet Cosine Similarity\")\n",
    "plt.ylabel(\"CoverHunter Cosine Similarity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"False Positives\", sorted by CoverHunter --> many falsely labeled negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_non_versions_cues = pd.merge(\n",
    "    pd.merge(data_neg.query(\"nlabel == 1\"), cue_map2, \n",
    "             on=\"yt_id\", how=\"left\").sort_values(by=\"mean_sim_neg_ch\", ascending=False),\n",
    "    data_seed.rename({\"title\": \"title_shs\"}, axis=1), on=\"set_id\", how=\"left\")\n",
    "\n",
    "\n",
    "data_non_versions_cues[[\"set_id\", \"reference_yt_id\", \"yt_id\", \"title\", \"title_shs\", \"mean_sim_neg_ch\", \n",
    "                        \"mean_sim_neg_cqt\", \"cue_list\"]].head(50) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_non_versions_cues[[\"set_id\", \"reference_yt_id\", \"yt_id\", \"title\", \"title_shs\", \"mean_sim_neg_ch\", \n",
    "                        \"mean_sim_neg_cqt\", \"cue_list\"]].sort_values(by=\"mean_sim_neg_cqt\", \n",
    "                                                                     ascending=False).head(50) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positives: No Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_cols_ch = torch.where(torch.tensor(rel_matrix_seedq_ch == 'yt-nomusic'), ypred_seedq_ch, torch.nan)\n",
    "data_ch[\"mean_sim_nomusic_ch\"] = torch.nanmean(sims_cols_ch, dim=0)\n",
    "\n",
    "# compute column wise means, CQTNet\n",
    "sims_cols_cqt = torch.where(torch.tensor(rel_matrix_seedq_cqt == 'yt-nomusic'), ypred_seedq_cqt, torch.nan)\n",
    "data_cqt[\"mean_sim_nomusic_cqt\"] = torch.nanmean(sims_cols_cqt, dim=0)\n",
    "\n",
    "# both CSI models\n",
    "data_nomusic = pd.merge(data_ch, data_cqt[[\"set_id\", \"yt_id\", \"mean_sim_nomusic_cqt\"]], how=\"left\", \n",
    "         on=[\"set_id\", \"yt_id\"])\n",
    "data_nomusic = data_nomusic[['set_id', 'reference_yt_id', 'yt_id', 'seed', 'sample_group',\n",
    "       'label', 'mean_sim_nomusic_ch', 'mean_sim_nomusic_cqt', 'mean_sim_ch', 'title', 'viewcount', 'duration',\n",
    "       'origin', 'description', 'upload_date', 'channel_name', 'ditto_pred', \n",
    "       're-move_pred', 'nlabel']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nomusic.query(\"nlabel == 0\").sort_values(by=\"mean_sim_nomusic_ch\", ascending=False).head(60).to_csv(\"data/2expert_nomusic_curation.csv\", sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"mean_sim_ch\").head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categories overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_expert = pd.read_csv(\"data/SHS-YT.csv\", sep=';').query(\"~nlabel_expert.isna()\")\n",
    "data_expert.category_expert = data_expert.category_expert.str.replace(\"video - same artist\", \"version - same artist\")\n",
    "data_expert.category_expert = data_expert.category_expert.str.strip()\n",
    "\n",
    "absolute_counts = data_expert.category_expert.value_counts()\n",
    "relative_counts = data_expert.category_expert.value_counts(normalize=True).round(2)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "result_df = pd.DataFrame({'Absolute Counts': absolute_counts, 'Relative Frequencies': relative_counts})\n",
    "result_df = result_df.sort_values(\n",
    "    by='Absolute Counts', ascending=False).reset_index().rename(\n",
    "    {\"index\": \"category\"}, axis=1)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uncertainties on actual covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_expert_versions = data_expert.query(\"nlabel > 1 and origin == 'expert'\")\n",
    "\n",
    "absolute_counts = data_expert_versions.category_expert.value_counts()\n",
    "relative_counts = data_expert_versions.category_expert.value_counts(normalize=True).round(2)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "result_df = pd.DataFrame({'Absolute Counts': absolute_counts, 'Relative Frequencies': relative_counts})\n",
    "result_df = result_df.sort_values(\n",
    "    by='Absolute Counts', ascending=False).reset_index().rename(\n",
    "    {\"index\": \"category\"}, axis=1)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uncertainties on actual non-covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_expert_non_versions = data_expert.query(\"nlabel <= 1 and origin == 'expert'\")\n",
    "\n",
    "absolute_counts = data_expert_non_versions.category_expert.value_counts()\n",
    "relative_counts = data_expert_non_versions.category_expert.value_counts(normalize=True).round(2)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "result_df = pd.DataFrame({'Absolute Counts': absolute_counts, 'Relative Frequencies': relative_counts})\n",
    "result_df = result_df.sort_values(\n",
    "    by='Absolute Counts', ascending=False).reset_index().rename(\n",
    "    {\"index\": \"category\"}, axis=1)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Worker uncertainties vs. model uncertainty\n",
    "Since the initial groups are rather small, we create new groups. Essentially, we distinguish between difficulties along the time dimension (eg. medleys, music and non-music, etc.) and difficulties along the audio/frequency dimension (eg. audio-quality, parody, backing track).\n",
    "- Time: \n",
    "- Frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, data_expert[['set_id', 'yt_id', 'label_expert', 'nlabel_expert', 'category_expert']],\n",
    "         on=[\"set_id\", \"yt_id\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_difficulties = ['video - music & non-music', 'video - multiple songs', \n",
    "                     'version - mashup/remix', 'version - medley']\n",
    "\n",
    "frequency_difficulties = ['video - background music', 'version - difficult', \n",
    "                          'video - audio-quality', 'version - parody', \n",
    "                          'video - drum only', 'version - backing track', 'version - same artist']\n",
    "\n",
    "def _get_difficulty_dim(x):\n",
    "    if x in time_difficulties:\n",
    "        return \"time\"\n",
    "    elif x in frequency_difficulties:\n",
    "        return \"frequency\"\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "data[\"difficulty_dimension\"] = data.category_expert.apply(_get_difficulty_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=data.query(\"nlabel >= 2\"), x=\"mean_sim_cqt\", y=\"mean_sim_ch\", \n",
    "                hue=\"difficulty_dimension\")\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.title(\"CQTNet vs. CoverHunter: Versions\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=data.query(\"nlabel >= 2\"), x=\"mean_sim_ch\", hue=\"difficulty_dimension\")        \n",
    "plt.title(\"CoverHunter\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=data.query(\"nlabel >= 2\"), x=\"mean_sim_cqt\", hue=\"difficulty_dimension\")        \n",
    "plt.title(\"CQTNet\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
