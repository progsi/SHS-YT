{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Benchmark\n",
    "\n",
    "We benchmark the models:\n",
    "- CQTNet (CSI)\n",
    "- Re-MOVE (CSI)\n",
    "- CoverHunter (CSI)\n",
    "- Fuzzy (Token Set Ratio -- Levensthein)\n",
    "- Ditto (Entity Matching)\n",
    "\n",
    "We benchmark on the following datasets:\n",
    "- SHS100K-Test\n",
    "- SHS100K-Test + YT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchmetrics\n",
    "We omit queries with no relevant items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.retrieval import RetrievalMAP, RetrievalHitRate\n",
    "\n",
    "#nDCG = RetrievalNormalizedDCG(empty_target_action='skip')\n",
    "mAP = RetrievalMAP(empty_target_action='skip')\n",
    "H10 = RetrievalHitRate(top_k=10, empty_target_action='skip')\n",
    "#MRR = RetrievalMRR(empty_target_action='skip')\n",
    "#P10 = RetrievalPrecision(top_k=10, empty_target_action='skip')\n",
    "#rP = RetrievalRPrecision(empty_target_action='skip')\n",
    "\n",
    "\n",
    "def ir_eval(preds, target, cls_based=False):\n",
    "    \"\"\"Computes various information retrieval metrics using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.tensor): similarity matrix MxN\n",
    "        target (torch.tensor): true relationships matrix MxN\n",
    "        k (int): number of top ranks for @k metrics\n",
    "    \"\"\"\n",
    "    # if target is ordinal, distinguish between ordinal and binary target\n",
    "    target_ord = None\n",
    "    if torch.max(target) > 1:\n",
    "        target_ord = target # ordinal\n",
    "        target = torch.where(target > 1, 1, 0) # binary\n",
    "    \n",
    "    # indexes for input structure for torchmetrics\n",
    "    m, n = target.shape\n",
    "    indexes = torch.arange(m).view(-1, 1).expand(-1, n)\n",
    "    \n",
    "    # metrics which only refer to the first rank\n",
    "    ir_dict = {\n",
    "        \"Queries\": int(len(target)),\n",
    "        \"Relevant Items\": int(torch.sum(target).item()),\n",
    "        #\"MRR\": MRR(preds, target, indexes).item(), \n",
    "        \"MR1\": utils.mr1(preds, target).item()\n",
    "    }\n",
    "    \n",
    "    # metrics which concern the top 10 or whole ranking\n",
    "    if not cls_based:\n",
    "        non_cls_evals = {\n",
    "            \"mAP\": mAP(preds, target, indexes).item(),\n",
    "            #\"nDCG_ord\": nDCG(preds, target_ord, indexes).item() if target_ord is not None \\\n",
    "            #    else torch.nan.item(), \n",
    "            #\"nDCG_bin\": nDCG(preds, target, indexes).item(), \n",
    "            #\"P@10\": P10(preds, target, indexes).item(),\n",
    "            \"HR10\": H10(preds, target, indexes).item(),\n",
    "            #\"rP\": rP(preds, target, indexes).item()\n",
    "            }\n",
    "        ir_dict.update(non_cls_evals)\n",
    "        \n",
    "    return dict(sorted(ir_dict.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Benchmark\n",
    "The overall benchmark of models on our dataset SHS-YT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "models = [\"coverhunter\", \"cqtnet\", \"remove\", \"ditto\", \"fuzzy\"]\n",
    "datasets = [\"SHS-SEED+YT\"]\n",
    "results = {}\n",
    "\n",
    "for model in tqdm(models):\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            df, target, preds = utils.get_dataset(model, dataset)\n",
    "            \n",
    "            ir_dict = ir_eval(preds, target)\n",
    "\n",
    "            results[model + '_' + dataset] = ir_dict\n",
    "             \n",
    "        except FileNotFoundError:\n",
    "            print(f\"No {dataset} predictions for {model}\")\n",
    "            continue \n",
    "\n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "models = [\"coverhunter\", \"cqtnet\", \"remove\", \"ditto\", \"fuzzy\"]\n",
    "datasets = [\"SHS-YT+2\"]\n",
    "results = {}\n",
    "\n",
    "for model in tqdm(models):\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            df, target, preds = utils.get_dataset_subset(model, dataset)\n",
    "            \n",
    "            ir_dict = ir_eval(preds, target)\n",
    "\n",
    "            results[model + '_' + dataset] = ir_dict\n",
    "             \n",
    "        except FileNotFoundError:\n",
    "            print(f\"No {dataset} predictions for {model}\")\n",
    "            continue \n",
    "\n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class-based Evaluation: MR1 and MRR\n",
    "We compare how different classes are ranked using the metrics MR1 and MRR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Classes\n",
    "Per relationship class, based on whether the candidate was in SHS-SEED or YT-CRAWL and its relevance label, we compute the metrics MRR and MR1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df, target, preds = utils.get_dataset(model, dataset)\n",
    "rels = utils.csi_relationship_matrix(df)\n",
    "\n",
    "# result dict\n",
    "results = {}\n",
    "\n",
    "for cls in [\"shs-pos\", \"yt-pos\", \"shs-neg\", \"yt-neg\", \"yt-nomusic\"]:\n",
    "    \n",
    "    # true relationship based on target class\n",
    "    cls_target = torch.tensor((rels == cls).astype(int))\n",
    "    results[cls] = ir_eval(preds, cls_target, cls_based=True)\n",
    "    \n",
    "results = pd.DataFrame(results).T\n",
    "results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguity Classes\n",
    "Per annotated ambiguity class, we compute the MRR and the MR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df, target, preds = utils.get_dataset(model, dataset)\n",
    "# binarize target\n",
    "target = torch.where(target > 1, 1, 0)\n",
    "\n",
    "# curated by expert\n",
    "df_curated = pd.read_csv(\"data/SHS-YT.csv\", sep=\";\").query(\"~category_expert.isna()\")\n",
    "\n",
    "# merge data\n",
    "df = pd.merge(df, df_curated[[\"set_id\", \"yt_id\", \"category_expert\"]], on=[\"set_id\", \"yt_id\"], how=\"left\")\n",
    "\n",
    "# set non-curated but seed \n",
    "df.loc[(df.seed & df.category_expert.isna()), 'category_expert'] = 'shs_seed'\n",
    "\n",
    "# all classes\n",
    "clss = df.category_expert.dropna().unique()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for cls in tqdm(clss):\n",
    "    \n",
    "    # to mask out if item at rank i is actually of cls\n",
    "    cls_mask = torch.tensor(((df.category_expert == cls).values).astype(int))\n",
    "    \n",
    "    # masked target\n",
    "    target_cls = target * cls_mask\n",
    "    \n",
    "    # mask to filter out 0-relevance queries\n",
    "    rel_mask = torch.sum(target_cls, dim=1) > 1\n",
    "    \n",
    "    if not sum(rel_mask) == 0:\n",
    "    \n",
    "        # limit queries on y-Axis of matrices so that the targets have the same length\n",
    "        _preds = preds[rel_mask]\n",
    "        _target = target[rel_mask]\n",
    "        _target_cls = target_cls[rel_mask]\n",
    "        \n",
    "        # compute results per class\n",
    "        ir_dict_cls = ir_eval(_preds, _target_cls, cls_based=True)\n",
    "        ir_dict_cls.pop('Queries')\n",
    "        ir_dict_cls = {key + '-CLS': value for key, value in ir_dict_cls.items()}\n",
    "        \n",
    "        # write results\n",
    "        ir_dict = ir_eval(_preds, _target, cls_based=True)\n",
    "        ir_dict.update(ir_dict_cls)\n",
    "        \n",
    "        results[cls] = ir_dict\n",
    "        \n",
    "results = pd.DataFrame(results).round(2).T.sort_values(by=\"Queries\", ascending=False)\n",
    "results[[\"MR1-CLS\", \"MRR-CLS\", \"Relevant Items-CLS\", \"MR1\", \"MRR\", \"Relevant Items\", \"Queries\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank-Analysis\n",
    "Due to variances in the number of ambiguity classes and relationship classes per clique, we conduct a rank analysis and first create a matrix with some rank metrics per query. We focus on the rank of the first relevant item per query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df, target, preds = utils.get_dataset(\"coverhunter\", dataset)\n",
    "df_curated = pd.read_csv(\"data/SHS-YT.csv\", sep=\";\").query(\"~category_expert.isna()\")\n",
    "df = pd.merge(df, df_curated[[\"set_id\", \"yt_id\", \"category_expert\"]], on=[\"set_id\", \"yt_id\"], how=\"left\")\n",
    "\n",
    "# binarize target\n",
    "target = torch.where(target > 1, 1, 0)\n",
    "\n",
    "# csi relationships\n",
    "rels = utils.csi_relationship_matrix(df)\n",
    "    \n",
    "\n",
    "# sort predictions tensor descendingly\n",
    "sort_tensor = torch.argsort(preds, descending=True)\n",
    "\n",
    "def first_ranks(sort_tensor, target):\n",
    "    \"\"\"compute first rank per row in tensor based on the sort_tensor.\n",
    "    :param sort_tensor (torch.tensor): the tensor with the sorting indices\n",
    "    :param target (torch.tensor): the binary target tensor\n",
    "    \"\"\"\n",
    "    # sort the target tensor\n",
    "    target_sorted = target.gather(1, sort_tensor)\n",
    "    \n",
    "    # get rank of first relevant, if no relevant --> nan\n",
    "    return torch.where(target_sorted.any(dim=1), target_sorted.argmax(dim=1), torch.tensor(np.nan))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set result_df\n",
    "df_rank_analysis = df[[\"set_id\", \"yt_id\",\"seed\"]]\n",
    "\n",
    "# write ranks per relationship class\n",
    "for cls_R in [\"shs-pos\", \"yt-pos\", \"yt-neg\", \"random-neg\", \"yt-nomusic\", ]:\n",
    "    \n",
    "    # target for csi relationships\n",
    "    target_R = torch.tensor((rels == cls_R).astype(int))\n",
    "    \n",
    "    df_rank_analysis[cls_R] = first_ranks(sort_tensor, target_R)\n",
    "    \n",
    "\n",
    "# ambiguities that apply for Non-Versions to consider how high non-relevant items rank\n",
    "negative_ambiguities = ['Song: Same Artist', 'Song: Same Genre', 'Video: Similar Metadata', 'Song: Similar']\n",
    "\n",
    "# write ranks per ambiguity class\n",
    "for cls_A in df.category_expert.dropna().unique():\n",
    "    \n",
    "    # target for ambiguity class\n",
    "    mask_A = torch.tensor(((df.category_expert == cls_A).values).astype(int))\n",
    "    \n",
    "    if cls_A in negative_ambiguities:\n",
    "        target_A = torch.tensor((rels == 'yt-neg').astype(int)) * mask_A\n",
    "    else:\n",
    "        target_A = target * mask_A\n",
    "    \n",
    "    df_rank_analysis[cls_A] = first_ranks(sort_tensor, target_A)\n",
    "\n",
    "\n",
    "df_rank_analysis[\"nrelevant_items\"] = torch.sum(target, axis=1)\n",
    "df_rank_analysis = df_rank_analysis.loc[df_rank_analysis.seed] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_R = torch.tensor((rels == \"random-neg\").astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_R = torch.tensor((rels == \"random-neg\").astype(int))\n",
    "torch.sum(target_R)/3282\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_R = torch.tensor((rels == \"shs-pos\").astype(int))\n",
    "torch.sum(target_R)/3282\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_R = torch.tensor((rels == \"yt-pos\").astype(int))\n",
    "torch.sum(target_R)/3282\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap: per Clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prepare data\n",
    "plt_data = df_rank_analysis.drop([\"yt_id\", \"seed\"], axis=1).groupby([\"set_id\"]).mean()[\n",
    "        [\"shs-pos\", \"yt-pos\", \"yt-neg\", \"random-neg\", \"yt-nomusic\"]\n",
    "    ]\n",
    "plt_data.columns = [\"SHS-Version\", \"YT-Version\", \"YT-Non-Version\", \"Random Non-Version\", \"No Music\"]\n",
    "\n",
    "# color map\n",
    "# cmap = 'RdBu_r'\n",
    "# cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "# cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "cmap = sns.color_palette(\"rocket_r\", as_cmap=True)\n",
    "\n",
    "\n",
    "# plot\n",
    "sns.heatmap(\n",
    "    np.sort(plt_data, axis=0),\n",
    "    vmax=50, \n",
    "    cmap=cmap\n",
    ")\n",
    "\n",
    "# axis labels\n",
    "plt.ylabel(\"Clique\")\n",
    "\n",
    "# ticks\n",
    "x_labels = [\"SHS-Version\", \"YT-Version\", \"YT-Non-Version\", \"Random Non-Version\", \"No Music\"]\n",
    "x_positions = range(len(x_labels))\n",
    "plt.xticks(x_positions, x_labels, rotation=30, ha=\"center\")\n",
    "plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figs/heatmap_mr1.pdf\")\n",
    "\n",
    "plt.title(\"MR1 per Relationship Class per Clique\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare R1 per class per Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.size'] = 20# Adjust the font size as needed\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Adjust the width and height as needed\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt_data = df_rank_analysis[[\"shs-pos\", \"yt-pos\", \"yt-neg\", \"random-neg\", \"yt-nomusic\"]]\n",
    "plt_data.columns =  [\"\\emph{Version} from SHS-SEED\", \"\\emph{Version} from SHS-YT\", \n",
    "                     \"\\emph{Non-Version} from SHS-YT\", \"Random \\emph{Non-Version}\", \"\\emph{No Music} from SHS-YT\"]\n",
    "plt_data.columns = [c + f\" ($N={plt_data[c].count()}$)\" for c in plt_data.columns]\n",
    "\n",
    "\n",
    "sns.ecdfplot(plt_data)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Rank of first item\")\n",
    "\n",
    "plt.savefig(\"figs/cdfs_cls_first_rank.pdf\")\n",
    "\n",
    "plt.title(f\"CDFs of first ranks per class per query (N={len(plt_data)})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = df_rank_analysis[['Song: Instrumental', 'Song: Vocal-Only', 'Video: Low Fidelity', 'Video: With Non-Music', 'Song: Medley', 'Song: Mashup/Remix',\n",
    "       'Video: Multiple Songs', 'Song: Difficult Cover', 'Song: Drum-Only',\n",
    "       'Song: Single Instrument', 'Song: Slowed/Spedup']]\n",
    "plt_data.columns = [c + f\" ($N={plt_data[c].count()}$)\" for c in plt_data.columns]\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))  # Adjust the width and height as needed\n",
    "\n",
    "\n",
    "# sns.ecdfplot(pd.melt(plt_data, var_name='Ambiguity', value_name='Value').dropna(), x=\"Value\", hue=\"Ambiguity\")\n",
    "ax = sns.ecdfplot(plt_data)\n",
    "\n",
    "sns.move_legend(ax, loc='lower left', bbox_to_anchor=(1, 0))\n",
    "\n",
    "plt.xlabel(\"Rank of first item\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig(\"figs/cdfs_ambiguity_versions_first_rank.pdf\")\n",
    "\n",
    "\n",
    "plt.title(f\"CDFs of first ranks per ambiguity class per query (N={len(plt_data)})\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Version Ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = df_rank_analysis[negative_ambiguities]\n",
    "plt_data.columns = [c + f\" ($N={plt_data[c].count()}$)\" for c in plt_data.columns]\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Adjust the width and height as needed\n",
    "\n",
    "sns.ecdfplot(plt_data)\n",
    "\n",
    "plt.xlabel(\"Rank of first item\")\n",
    "\n",
    "plt.savefig(\"figs/cdfs_ambiguity_nonversions_first_rank.pdf\")\n",
    "\n",
    "plt.title(f\"CDFs of first ranks per class per query (N={len(plt_data)})\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 1\n",
    "plt_data1 = df_rank_analysis[['Song: Instrumental', 'Song: Vocal-Only', 'Video: Low Fidelity', 'Video: With Non-Music', 'Song: Medley', 'Song: Mashup/Remix',\n",
    "       'Video: Multiple Songs', 'Song: Difficult Cover', 'Song: Drum-Only',\n",
    "       'Song: Single Instrument', 'Song: Slowed/Spedup']]\n",
    "plt_data1.columns = [c + f\" ($N={plt_data1[c].count()}$)\" for c in plt_data1.columns]\n",
    "\n",
    "# data 2\n",
    "plt_data2 = df_rank_analysis[negative_ambiguities]\n",
    "plt_data2.columns = [c + f\" ($N={plt_data2[c].count()}$)\" for c in plt_data2.columns]\n",
    "\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "# Plot the first ECDF plot on the first axis\n",
    "sns.ecdfplot(data=plt_data1, ax=axes[0])\n",
    "\n",
    "# Plot the second ECDF plot on the second axis\n",
    "sns.ecdfplot(data=plt_data2, ax=axes[1])\n",
    "\n",
    "# Set the x-axis label for both subplots\n",
    "axes[0].set_xlabel(\"Rank of first item\")\n",
    "axes[0].set_title(\"Version\")\n",
    "axes[1].set_xlabel(\"Rank of first item\")\n",
    "axes[1].set_title(\"Non-Version\")\n",
    "\n",
    "# Create a legend for the first subplot (axes[0])\n",
    "sns.move_legend(axes[0], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the multiplot to a file\n",
    "plt.savefig(\"figs/cdfs_ambiguity_multiplot.pdf\")\n",
    "\n",
    "# Show the multiplot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoverHunter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of items represented by yt_id\n",
    "m = np.tile(df.yt_id.values, (len(df.yt_id.values), 1))\n",
    "\n",
    "# rank by preds\n",
    "m = m[np.arange(m.shape[0])[:, np.newaxis], sort_tensor]\n",
    "\n",
    "pd.DataFrame(m, index=df.yt_id)[df.seed.values].to_csv(\"data/ranked_yt_ids.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "_, target, preds = utils.get_dataset(\"cqtnet\", dataset)\n",
    "\n",
    "# binarize target\n",
    "target = torch.where(target > 1, 1, 0)  \n",
    "\n",
    "# sort predictions tensor descendingly\n",
    "sort_tensor_cqt = torch.argsort(preds, descending=True)\n",
    "\n",
    "\n",
    "# matrix of items represented by yt_id\n",
    "m = np.tile(df.yt_id.values, (len(df.yt_id.values), 1))\n",
    "\n",
    "# rank by preds\n",
    "m = m[np.arange(m.shape[0])[:, np.newaxis], sort_tensor_cqt]\n",
    "\n",
    "df_m = pd.DataFrame(m, index=df.yt_id)[df.seed.values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are items ranked of Non-Music?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like a rolling stone\n",
    "yt_id_series = df_m.loc['DIevcLfERW4']\n",
    "print(yt_id_series.index[yt_id_series == 'YMe24aRwiwg'])\n",
    "\n",
    "# whats going on\n",
    "yt_id_series = df_m.loc['URoYcJ-2wks']\n",
    "print(yt_id_series.index[yt_id_series == 'PG6iJmbnOTY'])\n",
    "\n",
    "# natural woman\n",
    "yt_id_series = df_m.loc['EUtWB7Orkh8']\n",
    "print(yt_id_series.index[yt_id_series == 'YMe24aRwiwg'])\n",
    "\n",
    "\n",
    "yt_id_series = df_m.loc['EUtWB7Orkh8']\n",
    "print(yt_id_series.index[yt_id_series == 'YMe24aRwiwg'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity between Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import torch\n",
    "import os \n",
    "\n",
    "\n",
    "# Function to compute correlation between two tensors\n",
    "def compute_correlation(tensor1, tensor2, method=\"cosine\"):\n",
    "    \n",
    "    tensor1[tensor1 == -float('inf')] = 1\n",
    "    tensor2[tensor2 == -float('inf')] = 1\n",
    "\n",
    "    if method == \"cosine\":\n",
    "        # Handle zero vectors\n",
    "        if torch.all(tensor1 == 0) or torch.all(tensor2 == 0):\n",
    "            return 0.0\n",
    "        \n",
    "        # Normalize tensors\n",
    "        tensor1_normalized = tensor1 / torch.norm(tensor1)\n",
    "        tensor2_normalized = tensor2 / torch.norm(tensor2)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        corr = torch.nn.functional.cosine_similarity(tensor1_normalized.flatten(), tensor2_normalized.flatten(), dim=0)\n",
    "        return corr.item()\n",
    "    elif method == \"pearson\":\n",
    "            tensor1_normalized = tensor1 / torch.norm(tensor1)\n",
    "            tensor2_normalized = tensor2 / torch.norm(tensor2)\n",
    "            # Convert tensors to NumPy arrays\n",
    "            tensor1_np = tensor1_normalized.numpy()\n",
    "            tensor2_np = tensor2_normalized.numpy()\n",
    "            \n",
    "            # Compute Pearson correlation using scipy.stats\n",
    "            corr, _ = pearsonr(tensor1_np.flatten(), tensor2_np.flatten())\n",
    "            return corr\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported correlation method. Supported methods are 'cosine' and 'pearson'.\")\n",
    "\n",
    "# Create a dictionary of dictionaries to store correlations\n",
    "correlation_dict = {}\n",
    "models = os.listdir(\"data/preds/\")\n",
    "\n",
    "for i, model1 in enumerate(models):\n",
    "    correlation_dict[model1] = {}\n",
    "    for j, model2 in enumerate(models):\n",
    "        if i != j:  # Avoid self-comparisons\n",
    "            \n",
    "            preds1 = torch.load(os.path.join(\"data\", \"preds\", model1, \"SHS-SEED+YT\", \"ypred.pt\"))\n",
    "            preds2 = torch.load(os.path.join(\"data\", \"preds\", model2, \"SHS-SEED+YT\", \"ypred.pt\"))\n",
    "            \n",
    "            corr = compute_correlation(preds1, preds2)\n",
    "            correlation_dict[model1][model2] = round(corr, 2)\n",
    "\n",
    "# Create a DataFrame from the correlation dictionary\n",
    "df = pd.DataFrame.from_dict(correlation_dict, orient='index').sort_index(axis=1).sort_index()\n",
    "\n",
    "# Print the DataFrame\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
