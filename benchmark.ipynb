{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Benchmark\n",
    "\n",
    "We benchmark the models:\n",
    "- CQTNet (CSI)\n",
    "- Re-MOVE (CSI)\n",
    "- CoverHunter (CSI)\n",
    "- Fuzzy (Token Set Ratio -- Levensthein)\n",
    "- Ditto (Entity Matching)\n",
    "\n",
    "We benchmark on the following datasets:\n",
    "- SHS100K-Test\n",
    "- SHS100K-Test + YT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchmetrics\n",
    "We omit queries with no relevant items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.retrieval import RetrievalMAP, RetrievalHitRate\n",
    "\n",
    "mAP = RetrievalMAP(empty_target_action='skip')\n",
    "H10 = RetrievalHitRate(top_k=10, empty_target_action='skip')\n",
    "\n",
    "def ir_eval(preds, target, cls_based=False):\n",
    "    \"\"\"Computes various information retrieval metrics using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.tensor): similarity matrix MxN\n",
    "        target (torch.tensor): true relationships matrix MxN\n",
    "        k (int): number of top ranks for @k metrics\n",
    "    \"\"\"\n",
    "    # if target is ordinal, distinguish between ordinal and binary target\n",
    "    target_ord = None\n",
    "    if torch.max(target) > 1:\n",
    "        target_ord = target # ordinal\n",
    "        target = torch.where(target > 1, 1, 0) # binary\n",
    "    \n",
    "    # indexes for input structure for torchmetrics\n",
    "    m, n = target.shape\n",
    "    indexes = torch.arange(m).view(-1, 1).expand(-1, n)\n",
    "    \n",
    "    # metrics which only refer to the first rank\n",
    "    ir_dict = {\n",
    "        \"Queries\": int(len(target)),\n",
    "        \"Relevant Items\": int(torch.sum(target).item()),\n",
    "        #\"MRR\": MRR(preds, target, indexes).item(), \n",
    "        \"MR1\": utils.mr1(preds, target).item()\n",
    "    }\n",
    "    \n",
    "    # metrics which concern the top 10 or whole ranking\n",
    "    if not cls_based:\n",
    "        non_cls_evals = {\n",
    "            \"mAP\": mAP(preds, target, indexes).item(),\n",
    "            #\"nDCG_ord\": nDCG(preds, target_ord, indexes).item() if target_ord is not None \\\n",
    "            #    else torch.nan.item(), \n",
    "            #\"nDCG_bin\": nDCG(preds, target, indexes).item(), \n",
    "            #\"P@10\": P10(preds, target, indexes).item(),\n",
    "            \"HR10\": H10(preds, target, indexes).item(),\n",
    "            #\"rP\": rP(preds, target, indexes).item()\n",
    "            }\n",
    "        ir_dict.update(non_cls_evals)\n",
    "        \n",
    "    return dict(sorted(ir_dict.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Benchmark\n",
    "The overall benchmark of models on our dataset SHS-YT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "models = [\"coverhunter\", \"cqtnet\", \"remove\", \"ditto\", \"fuzzy\"]\n",
    "datasets = [\"SHS-SEED+YT\"]\n",
    "results = {}\n",
    "\n",
    "for model in tqdm(models):\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            df, target, preds = utils.get_dataset(model, dataset)\n",
    "            \n",
    "            ir_dict = ir_eval(preds, target)\n",
    "\n",
    "            results[model + '_' + dataset] = ir_dict\n",
    "             \n",
    "        except FileNotFoundError:\n",
    "            print(f\"No {dataset} predictions for {model}\")\n",
    "            continue \n",
    "\n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "models = [\"coverhunter\", \"cqtnet\", \"remove\", \"ditto\", \"fuzzy\"]\n",
    "datasets = [\"SHS-YT+2\"]\n",
    "results = {}\n",
    "\n",
    "for model in tqdm(models):\n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            df, target, preds = utils.get_dataset_subset(model, dataset)\n",
    "            \n",
    "            ir_dict = ir_eval(preds, target)\n",
    "\n",
    "            results[model + '_' + dataset] = ir_dict\n",
    "             \n",
    "        except FileNotFoundError:\n",
    "            print(f\"No {dataset} predictions for {model}\")\n",
    "            continue \n",
    "\n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class-based Evaluation: MR1 and MRR\n",
    "We compare how different classes are ranked using the metrics MR1 and MRR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Classes\n",
    "Per relationship class, based on whether the candidate was in SHS-SEED or YT-CRAWL and its relevance label, we compute the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "dataset = \"SHS-SEED+YT\"\n",
    "df, target, preds = utils.get_dataset(model, dataset)\n",
    "rels = utils.csi_relationship_matrix(df)\n",
    "\n",
    "# result dict\n",
    "results = {}\n",
    "\n",
    "for cls in [\"shs-pos\", \"yt-pos\", \"shs-neg\", \"yt-neg\", \"yt-nomusic\"]:\n",
    "    \n",
    "    # true relationship based on target class\n",
    "    cls_target = torch.tensor((rels == cls).astype(int))\n",
    "    results[cls] = ir_eval(preds, cls_target, cls_based=True)\n",
    "    \n",
    "results = pd.DataFrame(results).T\n",
    "results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguity Classes\n",
    "Per annotated ambiguity class, we compute the MRR and the MR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df, target, preds = utils.get_dataset(model, dataset)\n",
    "# binarize target\n",
    "target = torch.where(target > 1, 1, 0)\n",
    "\n",
    "# curated by expert\n",
    "df_curated = pd.read_csv(\"data/SHS-YT.csv\", sep=\";\").query(\"~category_expert.isna()\")\n",
    "\n",
    "# merge data\n",
    "df = pd.merge(df, df_curated[[\"set_id\", \"yt_id\", \"category_expert\"]], on=[\"set_id\", \"yt_id\"], how=\"left\")\n",
    "\n",
    "# set non-curated but seed \n",
    "df.loc[(df.seed & df.category_expert.isna()), 'category_expert'] = 'shs_seed'\n",
    "\n",
    "# all classes\n",
    "clss = df.category_expert.dropna().unique()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for cls in tqdm(clss):\n",
    "    \n",
    "    # to mask out if item at rank i is actually of cls\n",
    "    cls_mask = torch.tensor(((df.category_expert == cls).values).astype(int))\n",
    "    \n",
    "    # masked target\n",
    "    target_cls = target * cls_mask\n",
    "    \n",
    "    # mask to filter out 0-relevance queries\n",
    "    rel_mask = torch.sum(target_cls, dim=1) > 1\n",
    "    \n",
    "    if not sum(rel_mask) == 0:\n",
    "    \n",
    "        # limit queries on y-Axis of matrices so that the targets have the same length\n",
    "        _preds = preds[rel_mask]\n",
    "        _target = target[rel_mask]\n",
    "        _target_cls = target_cls[rel_mask]\n",
    "        \n",
    "        # compute results per class\n",
    "        ir_dict_cls = ir_eval(_preds, _target_cls, cls_based=True)\n",
    "        ir_dict_cls.pop('Queries')\n",
    "        ir_dict_cls = {key + '-CLS': value for key, value in ir_dict_cls.items()}\n",
    "        \n",
    "        # write results\n",
    "        ir_dict = ir_eval(_preds, _target, cls_based=True)\n",
    "        ir_dict.update(ir_dict_cls)\n",
    "        \n",
    "        results[cls] = ir_dict\n",
    "        \n",
    "results = pd.DataFrame(results).round(2).T.sort_values(by=\"Queries\", ascending=False)\n",
    "results[[\"MR1-CLS\", \"Relevant Items-CLS\", \"MR1\",\"Relevant Items\", \"Queries\"]]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shs-yt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
